{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import binary_crossentropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goals of this notebook\n",
    "\n",
    "For this notebook, I hope to compare my own implementation of Logistic Regression with one implemented by scikit-learn. Although, before I do this I need some data to work with. I will start with a simple linearly separable example, then will move to a more complex example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating our dataset\n",
    "\n",
    "I am not really sure the optimal approach to generating a dataset for classification. So, why not try a super naive yet simple approach.\n",
    "\n",
    "#### IMPLEMENTATION 1:\n",
    "For each class generate random weight vector (number of features + 1). Then, generate random data points (num samples, num features + 1). After projecting data points on to each classes weight vector, whichever class vector results in highest value (highest dot product) will be the samples class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset_for_classification(num_samples, num_classes, num_features, seed=42):\n",
    "    np.random.seed(seed)\n",
    "    X = np.random.uniform(-10, 10, (num_samples, num_features))\n",
    "    W = np.random.randn(num_features + 1, num_classes)\n",
    "    # add bias\n",
    "    X = np.hstack((X, np.ones(X.shape[0]).reshape(-1, 1)))\n",
    "    dots_plus_noise = np.dot(X, W) + np.random.normal(0, 0.1, (num_samples, num_classes))\n",
    "    y = np.argmax(dots_plus_noise, axis=1)\n",
    "    return X, y\n",
    "\n",
    "X, y = generate_dataset_for_classification(100, 2, 2)\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def generate_linearly_separable_data(num_samples, dim, distance_threshold):\n",
    "    # Mean of the two distributions\n",
    "    mean1 = np.zeros(dim)\n",
    "    mean2 = np.ones(dim)\n",
    "\n",
    "    # Covariance matrices for the distributions\n",
    "    cov = np.eye(dim) * 0.5  # Scaled identity matrix\n",
    "\n",
    "    # Generate samples\n",
    "    X1 = np.random.multivariate_normal(mean1, cov, num_samples)\n",
    "    X2 = np.random.multivariate_normal(mean2, cov, num_samples)\n",
    "\n",
    "    # Combine the samples\n",
    "    X = np.vstack((X1, X2))\n",
    "    y = np.hstack((np.zeros(num_samples), np.ones(num_samples)))\n",
    "\n",
    "    # Calculate midpoint between the means\n",
    "    midpoint = (mean1 + mean2) / 2\n",
    "\n",
    "    # any point along the line connecting the two means should be removed \n",
    "    v_hat = (mean2 - mean1).T / np.linalg.norm(mean2 - mean1)\n",
    "\n",
    "    projections = np.dot(X, v_hat).dot(v_hat)\n",
    "    X = X[np.linalg.norm(X - projections) > distance_threshold)]\n",
    "\n",
    "\n",
    "    # Remove points too close to the midpoint\n",
    "    distances = np.linalg.norm(X - midpoint, axis=1)\n",
    "    keep = distances > distance_threshold\n",
    "    X = X[keep]\n",
    "    y = y[keep]\n",
    "\n",
    "    return X, y\n",
    "\n",
    "# Generate data\n",
    "X, y = generate_linearly_separable_data(num_samples=100, dim=2, distance_threshold=0.5)\n",
    "\n",
    "# Plotting\n",
    "plt.scatter(X[y == 0][:, 0], X[y == 0][:, 1], color='red', label='Class 0')\n",
    "plt.scatter(X[y == 1][:, 0], X[y == 1][:, 1], color='blue', label='Class 1')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('Linearly Separable Data')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = np.array([[1,1],[1,2],[3,2],[3,3]])\n",
    "\n",
    "mid = np.mean(points, axis=0).reshape(-1, 1)\n",
    "ortho_mid = mid.T / np.linalg.norm(mid)\n",
    "\n",
    "print(ortho_mid)\n",
    "\n",
    "projects = np.dot(points, ortho_mid.T).dot(ortho_mid)\n",
    "\n",
    "plt.scatter(points[:, 0], points[:, 1])\n",
    "plt.scatter(mid[0], mid[1])\n",
    "plt.scatter(dif_mid[:, 0], dif_mid[:, 1])\n",
    "plt.scatter(projects[:, 0], projects[:, 1])\n",
    "plt.legend(['points', 'mid', 'dif_mid', 'projects'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_mono",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
